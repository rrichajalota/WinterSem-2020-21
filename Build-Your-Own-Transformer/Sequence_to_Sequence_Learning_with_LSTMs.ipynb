{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence to Sequence Learning with LSTMs.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdEpzuo-abSs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRuOxFMgabSu"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIg0BrNDKLeT"
      },
      "source": [
        "### Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UvpL8zfabSv"
      },
      "source": [
        "import os # NOTE: uncomment the following lines if the models aren't already loaded\n",
        "os.system('python -m spacy download en')\n",
        "os.system('python -m spacy download de')\n",
        "spacy_en = spacy.load('en')\n",
        "spacy_de = spacy.load('de')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx1wArwdabSy"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Pf8FKkabSz"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsHoVdxxabS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5cfa44-4102-444a-ade9-c9997e04e95a"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 690kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 172kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 158kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQXUmolpabS2",
        "outputId": "769ad767-0584-4e91-a806-25179c420ddf"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYUUwPDqabS3",
        "outputId": "49c66668-7541-4405-85f2-f3fbb3f82c8c"
      },
      "source": [
        "print(vars(train_data.examples[0])) #make sure the source sentence is reversed"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC8m4PkQabS4"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 3)\n",
        "TRG.build_vocab(train_data, min_freq = 3)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GsnvBQgabS4",
        "outputId": "71ce1d71-6af5-4699-a5f0-9c74a781d1f7"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 5376\n",
            "Unique tokens in target (en) vocabulary: 4556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMw1Xnf_abS5"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amOf8IjEabS5"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IrPq7pFabS6"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVZF2hJnabS9"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf6ILyWoabTA"
      },
      "source": [
        "### Decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vuj1inmabTB"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        #print(embedded.shape, hidden.shape, cell.shape)\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixDDFMyoabTC"
      },
      "source": [
        "### Seq2Seq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656NJnpuabTD"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.8):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTUSYUs2abTE"
      },
      "source": [
        "# Training the Seq2Seq Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWCUj9JmabTF"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POxCgOPrabTF",
        "outputId": "3f61e580-5c2c-4b9d-8c6a-489020f2c1ca"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5376, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.2)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(4556, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.2)\n",
              "    (fc_out): Linear(in_features=512, out_features=4556, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_uIHj0nabTG",
        "outputId": "642ee57d-262c-4fa1-d05a-cc1af1edd4db"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 12,236,236 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtnQ3wHjabTH"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq7TFhj0abTH"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVZC7axSabTI"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        if i == 40: break\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / 40 #len(iterator)"
      ],
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMRHoomabTJ"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8XUq3lNabTK"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0S8UkZwNaBg"
      },
      "source": [
        "class EarlyStopping():\n",
        "    \"\"\"\n",
        "    Early stopping to stop the training when the loss does not improve after\n",
        "    certain epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        :param patience: how many epochs to wait before stopping when loss is\n",
        "               not improving\n",
        "        :param min_delta: minimum difference between new loss and old loss for\n",
        "               new loss to be considered as an improvement\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    \n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                print('INFO: Early stopping')\n",
        "                self.early_stop = True"
      ],
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz5bUf2aabTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ef35c6-1b0f-4be5-8e94-a74a90c52ea6"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "early_stopping = EarlyStopping()\n",
        "training_loss= []\n",
        "val_loss= []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    training_loss.append(train_loss)\n",
        "    val_loss.append(valid_loss)\n",
        "    \n",
        "    # check for early stopping \n",
        "    early_stopping(valid_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        torch.save(model.state_dict(), 'model_early_stop.pt')\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        break\n",
        "\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 5s\n",
            "\tTrain Loss: 4.164 | Train PPL:  64.320\n",
            "\t Val. Loss: 5.376 |  Val. PPL: 216.069\n",
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 4.205 | Train PPL:  67.051\n",
            "\t Val. Loss: 5.225 |  Val. PPL: 185.873\n",
            "INFO: Early stopping counter 1 of 5\n",
            "Epoch: 03 | Time: 0m 5s\n",
            "\tTrain Loss: 4.195 | Train PPL:  66.321\n",
            "\t Val. Loss: 5.405 |  Val. PPL: 222.557\n",
            "INFO: Early stopping counter 2 of 5\n",
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 4.174 | Train PPL:  65.004\n",
            "\t Val. Loss: 5.545 |  Val. PPL: 255.917\n",
            "INFO: Early stopping counter 3 of 5\n",
            "Epoch: 05 | Time: 0m 5s\n",
            "\tTrain Loss: 4.129 | Train PPL:  62.138\n",
            "\t Val. Loss: 5.524 |  Val. PPL: 250.586\n",
            "INFO: Early stopping counter 4 of 5\n",
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 4.181 | Train PPL:  65.421\n",
            "\t Val. Loss: 5.395 |  Val. PPL: 220.248\n",
            "INFO: Early stopping counter 5 of 5\n",
            "INFO: Early stopping\n",
            "Epoch: 07 | Time: 0m 5s\n",
            "\tTrain Loss: 4.154 | Train PPL:  63.681\n",
            "\t Val. Loss: 5.382 |  Val. PPL: 217.400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep-LP6DmabTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd92856-fdea-4845-ce6c-10e7bc8a3ec0"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 5.210 | Test PPL: 183.158 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "j97qM9YNN3CB",
        "outputId": "c92250cc-b159-4d50-c243-f52eb2b84022"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "print('Saving loss plots...')\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(training_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('LSTM-based Seq2Seq training')\n",
        "plt.savefig(f\"val_train_loss.png\")\n",
        "plt.show()"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving loss plots...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94MWLLiQRMSX"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:]"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ3nvD0-PHg4"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "\n",
        "        \n",
        "        pred_trg = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4XiRHdxkwti",
        "outputId": "55251b6a-3f73-4041-ae77-0a6ebb045c08"
      },
      "source": [
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 2.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grc18IWRVF--",
        "outputId": "217be5b8-53a8-4cd8-c8ef-d7974ed8247b"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKGTvh-HVJYm",
        "outputId": "07abad72-84c0-4b18-8268-89456bf3a647"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def calculate_meteor(data, src_field, trg_field, model, device, max_len = 100):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    scores = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        #pred_trgs.append(pred_trg)\n",
        "        scores.append(nltk.translate.meteor_score.meteor_score([' '.join(pred_trg)], ' '.join(trg)))\n",
        "        \n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "meteor_score = calculate_meteor(test_data, SRC, TRG, model, device)\n",
        "\n",
        "print(f'METEOR score = {meteor_score*100:.2f}')\n"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "METEOR score = 24.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVIu5LS6_kTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9eb0b9-1c4f-4b13-f41b-0f3f8b1f3819"
      },
      "source": [
        "example_idx = 1800\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['src']\n",
        "trg = vars(train_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['.', 'stativ', 'einem', 'auf', 'kamera', 'einer', 'mit', 'fotografiert', 'kind', 'kleines', 'ein']\n",
            "trg = ['a', 'small', 'child', 'taking', 'a', 'picture', 'with', 'a', 'camera', 'on', 'a', 'tripod', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGrlVO2Fi12N",
        "outputId": "2546ab9a-5d7f-4e8b-84b3-9edd06e398b3"
      },
      "source": [
        "translation = translate_sentence(src, SRC, TRG, model, device)\n",
        "print(f'predicted trg = {\" \".join(translation)}')"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = a man in a blue shirt and a . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzBO9kjGiv1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99900138-fb40-4a53-d496-47794b8b7ecf"
      },
      "source": [
        "example_idx = 14\n",
        "\n",
        "src = vars(valid_data.examples[example_idx])['src']\n",
        "trg = vars(valid_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "translation = translate_sentence(src, SRC, TRG, model, device)\n",
        "print(f'predicted trg = {\" \".join(translation)}')"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['.', 'geige', 'ihrer', 'auf', 'lied', 'ein', 'spielt', 'frau', 'eine']\n",
            "trg = ['a', 'female', 'playing', 'a', 'song', 'on', 'her', 'violin', '.']\n",
            "predicted trg = a man in a blue shirt and a . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caMh7De_i5Ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad256ad-3f78-4f07-da74-b9eea751d99c"
      },
      "source": [
        "example_idx = 100\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['src']\n",
        "trg = vars(test_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "translation = translate_sentence(src, SRC, TRG, model, device)\n",
        "print(f'predicted trg = {\" \".join(translation)}')"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['.', 'zu', 'erfrischung', 'eine', 'coffee-shop', 'einem', 'in', 'bereitet', 'frau', 'glückliche', 'eine']\n",
            "trg = ['a', 'happy', 'woman', 'is', 'preparing', 'a', 'refreshment', 'at', 'a', 'coffee', 'shop', '.']\n",
            "predicted trg = a man in a blue shirt and a . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdBBojHWV0Ak"
      },
      "source": [
        ""
      ],
      "execution_count": 327,
      "outputs": []
    }
  ]
}